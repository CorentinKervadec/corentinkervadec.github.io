---
layout: default
---

I am a PhD student at [INSA Lyon](https://www.insa-lyon.fr/) financed by [Orange Labs](https://www.orange.com/fr/accueil) in the [Imagine team (LIRIS)](https://liris.cnrs.fr/equipe/imagine) under the direction of [Christian Wolf](https://perso.liris.cnrs.fr/christian.wolf/).

My thesis is also co-supervised by [Grigory Antipov](https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl) and [Moez Baccouche](https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl) from Orange Labs.

My work focuses on Deep Learning applied to Vision and Language, with a special interest on Visual Reasoning and Visual Question Answering (VQA).

# News

* --June-- December 2020 (updated version): New paper on Arxiv! ["Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?"](https://128.84.21.199/pdf/2006.05121.pdf)

* June 2020: New paper on Arxiv! ["Estimating semantic structure for the VQA answer space"](https://128.84.21.199/pdf/2006.05726.pdf)

* January 2020: One paper accepted at [ECAI20](http://ecai2020.eu/)! ["Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks."](https://arxiv.org/pdf/1912.03063.pdf)

* May 2019: One paper accepted at [FG2019](https://fg2019.org/)! ["The Many Variations of Emotion."](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8756560)

* July 2018: One paper accepted at the IAHFAR wotkshop hosted at [BMVC18](http://bmvc2018.org/index.html)! ["CAKE: Compact and Accurate K-dimensional representation of Emotion.](http://bmvc2018.org/contents/workshops/iahfar2018/0037.pdf)

* October 2018: Sarting my PhD at INSA Lyon & Orange Labs under the direction of [Christian Wolf](https://perso.liris.cnrs.fr/christian.wolf/) and co-supervised by [Grigory Antipov](https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl) and [Moez Baccouche](https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl).

* June 2018: Rank 3th at the [Emotion in the Wild 2018 challenge](https://sites.google.com/view/emotiw2018) hosted at [ICMI18](https://icmi.acm.org/2018/)! ["An Occam's Razor View on Learning Audiovisual Emotion Recognition with Small Training Sets."](https://dl.acm.org/doi/pdf/10.1145/3242969.3264980)  

* March 2018, Starting a Master's internship at [Orange Labs](https://www.orange.com/fr/accueil).

# Publications
 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>     
                    <b><p style="font-size:18px;"><a href="https://128.84.21.199/pdf/2006.05121.pdf"> Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?</a></p></b>
                    <p align="center"><img src="gqa_ood/teaser-b-v4.png" alt="blind-date" width="66%">
                     <p>
                            <br>
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>Arxiv</em>, 2020 &nbsp;
                            <br>
                            <a href="https://128.84.21.199/pdf/2006.05121.pdf">PDF</a>
                            /
                            <a href="https://128.84.21.199/abs/2006.05121">arXiv</a>
                            /
                            <a href="https://github.com/gqa-ood/GQA-OOD/tree/master/code">Code</a>
                            /
                            <a href="https://github.com/gqa-ood/GQA-OOD">Benchmark</a>
                        </p>
                        <p>  We propose <a href="https://github.com/gqaood/GQA-OOD">GQA-OOD</a>, a new benchmark to evaluate VQA in out-of-distribution settings by reorganizing the GQA dataset, taylored for each sample (question group), targeting research in bias reduction in VQA.
                        </p>
                <tbody>
                   <b><p style="font-size:18px;"><a href="https://128.84.21.199/pdf/2006.05726.pdf">
                    Estimating semantic structure for the VQA answer space</a></p></b>
                   <p align="center"><img src="semantic_vqa/general.png" alt="blind-date"                                width="66%"></p>
                        <p>
                            <br>
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>Arxiv</em>, 2020 &nbsp;
                            <br>
                            <a href="https://128.84.21.199/pdf/2006.05726.pdf">PDF</a>
                            /
                            <a href="https://128.84.21.199/abs/2006.05726">arXiv</a>
                        </p>
                        <p>  Semantic loss for VQA adding structure to the VQA answer space estimated from redundancy in annotations, questioning the classification approach to VQA.
                        </p>
                <tbody>
                 <b><p style="font-size:18px;"><a href="http://ecai2020.eu/papers/1241_paper.pdf">
                                Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks</a></p></b>
                 <p align="center"><img src="word_object_alignment/fig_teaser.png" alt="blind-date"
                                        width="66%"></p>
                        <p>
                            <br>
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>ECAI</em>, 2020 &nbsp;
                            <br>
                            <a href="http://ecai2020.eu/papers/1241_paper.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1912.03063">arXiv</a>
                            /
                            <a href="https://youtu.be/SW8J0G8AbgY">video</a>
                            /
                            <a href="./bib/KervadecWOA_ECAI_20.txt">bibtex</a>
                        </p>
                        <p>  We introduce a weakly supervised word-object alignment inside BERT-like Vision-Language encoders, allowing to model fine-grained entity relations and improve visual reasoning capabilities.
                        </p>   
                <tbody>
                 <b><p style="font-size:18px;"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8756560&tag=1">
                                The Many Variations of Emotion</a></p></b>
                 <p align="center"><img src="many_variations/many_variations.PNG" alt="blind-date"
                                        width="66%"></p>
                        <p>
                            <br>
                            <a href="https://scholar.google.fr/citations?user=ve7hYuQAAAAJ&">Valentin Vielzeuf</a>,
                            <strong>Corentin Kervadec</strong>,
                            Stéphane Pateux,
                            <a href="https://frederic-jurie.github.io/">Frederic Jurie</a>
                            <br>
                            <em>FG</em>, 2019 &nbsp;
                            <br>
                            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8756560&tag=1">PDF</a>
                            /
                            <a href="./bib/VielzeufMAny_FG_19.txt">bibtex</a>
                        </p> We present a novel approach for changing facial expression in images by the use of a continuous latent space of emotion.
                        <p>  
                        </p>   
                <tbody>
                 <b><p style="font-size:18px;"><a href="http://bmvc2018.org/contents/workshops/iahfar2018/0037.pdf">
                                CAKE: Compact and Accurate K-dimensional representation of Emotion</a></p></b>
                 <p align="center"><img src="cake/cake_viz.PNG" alt="blind-date"
                                        width="66%"></p>
                        <p>
                            <br>
                            <strong>Corentin Kervadec*</strong>, 
                            <a href="https://scholar.google.fr/citations?user=ve7hYuQAAAAJ&">Valentin Vielzeuf*</a>,
                            Stéphane Pateux,
                            <a href="https://lechervy.users.greyc.fr/">Alexis Lechervy,</a>
                            <a href="https://frederic-jurie.github.io/">Frederic Jurie</a>
                            <br>
                            <em>IAHFAR workshop (BMVC)</em>, 2018 &nbsp;
                            <br>
                            <a href="http://bmvc2018.org/contents/workshops/iahfar2018/0037.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1807.11215">arXiv</a>
                            / 
                            <a href="./bib/KervadecCake_IAHFAR_18.txt">bibtex</a>
                        </p> We propose CAKE, a 3-dimensional representation of emotion learned in a multi-domain fashion, achieving accurate emotion recognition on several public datasets
                        <p>  
                        </p>   
                <tbody>
                 <b><p style="font-size:18px;"><a href="https://dl.acm.org/doi/pdf/10.1145/3242969.3264980">
                                An occam's razor view on learning audiovisual emotion recognition with small training sets</a></p></b>
                        <p>
                            <a href="https://scholar.google.fr/citations?user=ve7hYuQAAAAJ&">Valentin Vielzeuf</a>,
                            <strong>Corentin Kervadec</strong>, 
                            Stéphane Pateux,
                            <a href="https://lechervy.users.greyc.fr/">Alexis Lechervy,</a>
                            <a href="https://frederic-jurie.github.io/">Frederic Jurie</a>
                            <br>
                            <em>EmotiW challenge (ICMI)</em>, 2018 &nbsp;
                            <br>
                            <a href="https://dl.acm.org/doi/pdf/10.1145/3242969.3264980">PDF</a>
                            / 
                            <a href="./bib/VielzeufOccam_ICMI_18.txt">bibtex</a>
                        </p>  A light-weight and accurate deep neural model for audiovisual emotion recognition. We ranked 3th at the Emotion in the Wild 2018 challenge.
                        <p>  
                        </p>   
                <tbody>
