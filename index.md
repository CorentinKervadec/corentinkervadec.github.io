---
layout: default
---

### Corentin Kervadec
#### Researcher in Artificial Intelligence

---

Welcome to my webpage. I am a postdoc researcher in the Department of Linguistics at Pompeu Fabra University, within the [COLT](https://www.upf.edu/web/colt) lab. I study artificial intelligence, and in particular Large Language Models (LLMs). 

* **[2022-Now]** Postdoctoral member of the [ALiEN](https://marcobaroni.org/alien/) research program lead by Prof. Marco Baroni in the [COLT](https://www.upf.edu/web/colt) group at UPF (Barcelona, Spain). I conduct research on LLM Interpretability.

* **[2021-2022]** NLP Research Scientist, Orange Labs.

* **[2018-2021]** PhD, Orange Labs and [INSA Lyon](https://www.insa-lyon.fr/) (France). Thesis: *"Bias and Reasoning in Visual Question Answering."* Under the direction of [Christian Wolf](https://chriswolfvision.github.io/www/) and co-supervised by [Grigory Antipov](https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl) and [Moez Baccouche](https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl).

* **[2013-2018]** **M.Eng in Electronics & Computer Engineering**, INSA Engineering School (Rennes, France). 


<!---üëã I am a PhD candidate at [INSA Lyon](https://www.insa-lyon.fr/) financed by [Orange Labs](https://www.orange.com/fr/accueil) in the [Imagine team (LIRIS)](https://liris.cnrs.fr/equipe/imagine) under the direction of [Christian Wolf](https://perso.liris.cnrs.fr/christian.wolf/).

My thesis is also co-supervised by [Grigory Antipov](https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl) and [Moez Baccouche](https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl) from Orange Labs.--->


<!---üß† My PhD thesis is titled [*Bias and Reasoning in Visual Question Answering*](https://tel.archives-ouvertes.fr/tel-03584234v2/document) and focuses on Deep Learning applied to Vision and Language. I investigated how decisions made by a neural network trained on the Visual Question Answering (VQA) task are impacted by biases found in the training data.--->

<!---üì¢ **I am looking for a postdoc in ML. I would enjoy working on shortcut learning and/or reasoning models. Please, feel free to contact me ([CV](cv_kervadec_corentin_06_2021.pdf)).** --->

<!-- üçä I am currently conducting research on NLP x Neurosciences at Orange Innovation. -->


<!--# News

üëΩ *November 2022*: Sarting a postdoc on the [ALiEN](https://marcobaroni.org/alien/) project in [COLT](https://www.upf.edu/web/colt) group at UPF (Barcelona, Spain).

ü•á *October 2022*: I have been selected as an outstanding reviewer for ECCV'22 !

ü•á *May 2022*: I have been selected as an outstanding reviewer for CVPR'22 !

üë®‚Äçüéì *December 2021*: I successfully defended my PhD titled [*Bias and Reasoning in Visual Question Answering*](https://tel.archives-ouvertes.fr/tel-03584234v2/document)!

üìú *September 2021*: 1 paper accepted at [NeurIPS2021](https://neurips.cc/)! [Supervising the Transfer of Reasoning Patterns in VQA](https://openreview.net/forum?id=kqYiS7HEWfZ)

ü•á *September 2021*: I have been selected as an outstanding reviewer for ICCV'21 (top 5% students)!

üìú *July 2021*: 1 paper accepted at [IEEE VIS2021](http://ieeevis.org/year/2021/welcome)! [VisQA: X-raying Vision and Language Reasoning in Transformers](https://arxiv.org/abs/2104.00926)

üë®‚Äçüè´ *June 2021*: I presented a poster about biases and reasoning at the [VQA workshop](https://visualqa.org/workshop.html) at CVPR'21. Watch the [video](https://youtu.be/ABY2InY-RaE) and the [poster](posters/biases_reasoning_vqa.pdf)!

üë®‚Äçüè´ *May 2021*: I was invited to give a talk about biases and reasoning in VQA at ["Devil is in the Deeptails"](https://project.inria.fr/ml3ri/dissemination-and-communication/deeptails/) ([slides](https://project.inria.fr/ml3ri/files/2021/06/slides-corentin.pdf) and [video](https://www.youtube.com/watch?v=i8BTiR8rs5Q)).

üë®‚Äçüè´ *April 2021*: I gave a talk about VQA and visual reasoning at the GdR ISIS ["Explicabilit√© et Interpr√©tabilit√© des m√©thodes d'Intelligence Artificielle pour la classification et compr√©hension des sc√®nes visuelles"](http://www.gdr-isis.fr/index.php?page=compte-rendu&idreunion=446) meeting. Slides are available [here](https://github.com/CorentinKervadec/corentinkervadec.github.io/blob/master/slides/GdR_ISIS___Roses_Reasoning__slides_Kervadec_042021.pdf).

üìú *Mars 2021*: 2 papers accepted at [CVPR2021](http://cvpr2021.thecvf.com/)! ["Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?"](https://arxiv.org/abs/2006.05121) and ["How Transferable are Reasoning Patterns in VQA?"](https://arxiv.org/abs/2104.03656) (checkout our online demo [here](https://reasoningpatterns.github.io/)!)

üìú *June 2020*: New paper on Arxiv! ["Estimating semantic structure for the VQA answer space"](https://arxiv.org/abs/2006.05726)

üìú *January 2020*: One paper accepted at [ECAI20](http://ecai2020.eu/)! ["Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks."](https://arxiv.org/pdf/1912.03063.pdf)

üìú *May 2019*: One paper accepted at [IEEE FG2019](https://fg2019.org/)! ["The Many Variations of Emotion."](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8756560)

üë®‚Äçüéì *October 2018*: Sarting my PhD at INSA Lyon & Orange Labs under the direction of [Christian Wolf](https://perso.liris.cnrs.fr/christian.wolf/) and co-supervised by [Grigory Antipov](https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl) and [Moez Baccouche](https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl).

üìú *July 2018*: One paper accepted at the IAHFAR wotkshop hosted at [BMVC18](http://bmvc2018.org/index.html)! ["CAKE: Compact and Accurate K-dimensional representation of Emotion.](http://bmvc2018.org/contents/workshops/iahfar2018/0037.pdf)

ü•â *June 2018*: Rank 3th at the [Emotion in the Wild 2018 challenge](https://sites.google.com/view/emotiw2018) hosted at [ICMI18](https://icmi.acm.org/2018/)! ["An Occam's Razor View on Learning Audiovisual Emotion Recognition with Small Training Sets."](https://dl.acm.org/doi/pdf/10.1145/3242969.3264980)  

üë®‚Äçüéì *March 2018*, Starting a Master's internship at [Orange Labs](https://www.orange.com/fr/accueil).

# Publications
 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tbody>     
                    <b><p style="font-size:18px;"><a href="https://tel.archives-ouvertes.fr/tel-03584234v2/document"> Bias and Reasoning in Visual Question Answering</a></p></b>
                    <p align="center"><img src="thesis/first.PNG" alt="blind-date" width="80%">
                     <p>
                            <br>
                            <strong>Corentin Kervadec</strong>,
                            <br>
                            <em>PhD, INSA Lyon</em>, 2021 &nbsp;
                            <br>
                            <a href="https://tel.archives-ouvertes.fr/tel-03584234v2/document">PDF</a>
                        </p>
                        <p> Despite impressive improvement made by deep learning approaches, VQA models are notorious for their tendency to rely on dataset biases. In this thesis, we adress the VQA task through the prism of biases and reasoning, following the <em>motto</em>: evaluate, analyse, and improve.
                        </p>
                  <tbody>     
                    <b><p style="font-size:18px;"><a href="https://openreview.net/forum?id=kqYiS7HEWfZ"> Supervising the Transfer of Reasoning Patterns in VQA</a></p></b>
                    <p align="center"><img src="progqa/fig2_v4.png" alt="blind-date" width="80%">
                     <p>
                            <br>
                            <strong>Corentin Kervadec*</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf*</a>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="https://scholar.google.fr/citations?user=KOXeslUAAAAJ&hl">Madiha Nadri</a>,
                            <br>
                            <em>NeurIPS</em>, 2021 &nbsp;
                            <br>
                            <a href="https://openreview.net/pdf?id=kqYiS7HEWfZ">PDF</a>
                            /
                            <a href="https://openreview.net/forum?id=kqYiS7HEWfZ">OpenReview</a>
                        </p>
                        <p>  We propose a method for knowledge transfer in VQA based on a regularization term in our loss function, supervising the sequence of required reasoning operations. We provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses.
                        </p>
                 <tbody>     
                    <b><p style="font-size:18px;"><a href="https://arxiv.org/abs/2104.00926"> VisQA: X-raying Vision and Language Reasoning in Transformers</a></p></b>
                    <p align="center"><img src="visqa/visqa.png" alt="blind-date" width="80%">
                     <p>
                            <br>
                            <a href="https://theo-jaunet.github.io/">Theo Jaunet</a>,
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="https://romain.vuillemot.net/">Romain Vuillemot</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>IEEE VIS</em>, 2021 &nbsp;
                            <br>
                            <a href="https://arxiv.org/pdf/2104.00926.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/2104.00926">Arxiv</a>
                            /
                            <a href="https://github.com/Theo-Jaunet/VisQA">Github</a>
                            /
                            <a href="https://visqa.liris.cnrs.fr/">Online Demo!</a>
                        </p>
                        <p>  We introduce VisQA, a visual analytics tool that explores the question of reasoning vs. bias exploitation in Visual Question Answering systems. Try our interactive tool <a href="https://visqa.liris.cnrs.fr/">here</a>!
                        </p>
                <tbody>     
                    <b><p style="font-size:18px;"><a href="https://arxiv.org/abs/2104.03656"> How Transferable are Reasoning Patterns in VQA?</a></p></b>
                    <p align="center"><img src="reasoning_patterns/teaser.png" alt="blind-date" width="90%">
                     <p>
                            <br>
                            <strong>Corentin Kervadec*</strong>,
                            <a href="https://theo-jaunet.github.io/">Theo Jaunet*</a>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="https://romain.vuillemot.net/">Romain Vuillemot</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>CVPR</em>, 2021 &nbsp;
                            <br>
                            <a href="https://arxiv.org/pdf/2104.03656.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/2104.03656">Arxiv</a>
                            /
                            <a href="https://youtu.be/fN1m7b6u3-E">Video</a>
                            /
                            <a href="posters/poster_reasoning_v2.pdf">Poster</a>
                            /
                            <a href="https://reasoningpatterns.github.io/">Online Demo!</a>
                        </p>
                        <p>  Noise and uncertainties in visual inputs are the main bottleneck in VQA, preventing successful learning of reasoning capacities. In a deep analysis, we show that oracle models trained on noiseless visual data, tend to depend significantly less on bias exploitation (<a href="https://reasoningpatterns.github.io/">checkout our interactive tool</a>). In this, paper we demonstrate the feasability and the effectiveness of transfering learned reasoning patterns from oracle to real data based models.
                        </p>
                <tbody>     
                    <b><p style="font-size:18px;"><a href="https://128.84.21.199/pdf/2006.05121.pdf"> Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?</a></p></b>
                    <p align="center"><img src="gqa_ood/teaser-girl-v8.png" alt="blind-date" width="66%">
                     <p>
                            <br>
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>CVPR</em>, 2021 &nbsp;
                            <br>
                            <a href="https://arxiv.org/pdf/2006.05121.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/2006.05121">arXiv</a>
                            /
                            <a href="https://github.com/gqa-ood/GQA-OOD/tree/master/code">Code</a>
                            /
                            <a href="https://github.com/gqa-ood/GQA-OOD">Benchmark</a>
                            /
                            <a href="https://youtu.be/TAPS715WN7o">Video</a>
                            /
                            <a href="posters/poster_roses_v2.pdf">Poster</a>
                        </p>
                        <p>  We propose <a href="https://github.com/gqa-ood/GQA-OOD">GQA-OOD</a>, a new benchmark to evaluate VQA in out-of-distribution settings by reorganizing the GQA dataset, taylored for each sample (question group), targeting research in bias reduction in VQA.
                        </p>
                <tbody>
                   <b><p style="font-size:18px;"><a href="https://128.84.21.199/pdf/2006.05726.pdf">
                    Estimating semantic structure for the VQA answer space</a></p></b>
                   <p align="center"><img src="semantic_vqa/general.png" alt="blind-date"                                width="66%"></p>
                        <p>
                            <br>
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>Arxiv</em>, 2020 &nbsp;
                            <br>
                            <a href="https://arxiv.org/pdf/2006.05726.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/2006.05726">arXiv</a>
                        </p>
                        <p>  Semantic loss for VQA adding structure to the VQA answer space estimated from redundancy in annotations, questioning the classification approach to VQA.
                        </p>
                <tbody>
                 <b><p style="font-size:18px;"><a href="http://ecai2020.eu/papers/1241_paper.pdf">
                                Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks</a></p></b>
                 <p align="center"><img src="word_object_alignment/fig_teaser.png" alt="blind-date"
                                        width="66%"></p>
                        <p>
                            <br>
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>ECAI</em>, 2020 &nbsp;
                            <br>
                            <a href="http://ecai2020.eu/papers/1241_paper.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1912.03063">arXiv</a>
                            /
                            <a href="https://youtu.be/SW8J0G8AbgY">video</a>
                            /
                            <a href="./bib/KervadecWOA_ECAI_20.txt">bibtex</a>
                        </p>
                        <p>  We introduce a weakly supervised word-object alignment inside BERT-like Vision-Language encoders, allowing to model fine-grained entity relations and improve visual reasoning capabilities.
                        </p>   
                <tbody>
                 <b><p style="font-size:18px;"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8756560&tag=1">
                                The Many Variations of Emotion</a></p></b>
                 <p align="center"><img src="many_variations/many_variations.PNG" alt="blind-date"
                                        width="66%"></p>
                        <p>
                            <br>
                            <a href="https://scholar.google.fr/citations?user=ve7hYuQAAAAJ&">Valentin Vielzeuf</a>,
                            <strong>Corentin Kervadec</strong>,
                            St√©phane Pateux,
                            <a href="https://frederic-jurie.github.io/">Frederic Jurie</a>
                            <br>
                            <em>IEEE FG</em>, 2019 &nbsp;
                            <br>
                            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8756560&tag=1">PDF</a>
                            /
                            <a href="./bib/VielzeufMAny_FG_19.txt">bibtex</a>
                        </p> We present a novel approach for changing facial expression in images by the use of a continuous latent space of emotion.
                        <p>  
                        </p>   
                <tbody>
                 <b><p style="font-size:18px;"><a href="http://bmvc2018.org/contents/workshops/iahfar2018/0037.pdf">
                                CAKE: Compact and Accurate K-dimensional representation of Emotion</a></p></b>
                 <p align="center"><img src="cake/cake_viz.PNG" alt="blind-date"
                                        width="66%"></p>
                        <p>
                            <br>
                            <strong>Corentin Kervadec*</strong>, 
                            <a href="https://scholar.google.fr/citations?user=ve7hYuQAAAAJ&">Valentin Vielzeuf*</a>,
                            St√©phane Pateux,
                            <a href="https://lechervy.users.greyc.fr/">Alexis Lechervy,</a>
                            <a href="https://frederic-jurie.github.io/">Frederic Jurie</a>
                            <br>
                            <em>IAHFAR workshop (BMVC)</em>, 2018 &nbsp;
                            <br>
                            <a href="http://bmvc2018.org/contents/workshops/iahfar2018/0037.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1807.11215">arXiv</a>
                            / 
                            <a href="./bib/KervadecCake_IAHFAR_18.txt">bibtex</a>
                        </p> We propose CAKE, a 3-dimensional representation of emotion learned in a multi-domain fashion, achieving accurate emotion recognition on several public datasets
                        <p>  
                        </p>   
                <tbody>
                 <b><p style="font-size:18px;"><a href="https://dl.acm.org/doi/pdf/10.1145/3242969.3264980">
                                An occam's razor view on learning audiovisual emotion recognition with small training sets</a></p></b>
                        <p>
                            <a href="https://scholar.google.fr/citations?user=ve7hYuQAAAAJ&">Valentin Vielzeuf</a>,
                            <strong>Corentin Kervadec</strong>, 
                            St√©phane Pateux,
                            <a href="https://lechervy.users.greyc.fr/">Alexis Lechervy,</a>
                            <a href="https://frederic-jurie.github.io/">Frederic Jurie</a>
                            <br>
                            <em>EmotiW challenge (ICMI)</em>, 2018 &nbsp;
                            <br>
                            <a href="https://dl.acm.org/doi/pdf/10.1145/3242969.3264980">PDF</a>
                            / 
                            <a href="./bib/VielzeufOccam_ICMI_18.txt">bibtex</a>
                        </p>  A light-weight and accurate deep neural model for audiovisual emotion recognition. We ranked 3th at the Emotion in the Wild 2018 challenge.
                        <p>  
                        </p>   
                <tbody>
-->
