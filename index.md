---
layout: default
---

Page in construction...

# About

I am a PhD student at [INSA Lyon](https://www.insa-lyon.fr/) financed by [Orange Labs](https://www.orange.com/fr/accueil) in the [Imagine team (LIRIS)](https://liris.cnrs.fr/equipe/imagine) under the direction of [Christian Wolf](https://perso.liris.cnrs.fr/christian.wolf/).

My thesis is also co-supervised by [Grigory Antipov](https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl) and [Moez Baccouche](https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl) from Orange Labs.

My work focuses on Deep Learning applied to Vision and Language, with a special interest on Visual Question Answering (VQA).

# News

* January 2020: One paper accepted at [ECAI20](http://ecai2020.eu/)! ["Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks."](https://arxiv.org/pdf/1912.03063.pdf)

* May 2019: One paper accepted at [FG2019](https://fg2019.org/)! ["The Many Variations of Emotion."](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8756560)

* July 2018: One paper accepted at the IAHFAR wotkshop hosted at [BMVC18](http://bmvc2018.org/index.html)! ["CAKE: Compact and Accurate K-dimensional representation of Emotion.](http://bmvc2018.org/contents/workshops/iahfar2018/0037.pdf)

* October 2018: Sarting my PhD.

* June 2018: Rank 3th at the [Emotion in the Wild 2018 challenge](https://sites.google.com/view/emotiw2018) hosted at [ICMI18](https://icmi.acm.org/2018/)! ["An Occam's Razor View on Learning Audiovisual Emotion Recognition with Small Training Sets."](https://dl.acm.org/doi/pdf/10.1145/3242969.3264980)  

* March 2018, Starting a Master's internship at [Orange Labs](https://www.orange.com/fr/accueil).

# Publications

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>               
                    <td width="25%"><img src="images/cf.png" alt="blind-date"
                                         width="200"
                                         height="105"></td>
                    <td width="75%" valign="top">
                        <p>
                            <a href="https://arxiv.org/abs/1909.12000">
                                <papertitle>CoPhy: Counterfactual Learning of Physical Dynamics
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="https://nneverova.github.io/">Natalia Neverova</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
                            <a href="http://www.cs.sfu.ca/~mori/">Greg Mori</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>ICLR</em>, 2020 &nbsp; <font color="red"><strong>(Spotlight presentation)</strong></font>
                             <!--<em>International Conference on Learning Representations-->
                                <!--(<strong>ICLR</strong>)</em>, 2020 <strong>(spotlight)</strong>-->
                            <br>
                            <a href="https://arxiv.org/pdf/1909.12000.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1909.12000">arXiv</a>
                            /
                            <a href="https://github.com/fabienbaradel/cophy">Code-Dataset</a>
                            /
                            <a href="https://youtu.be/95nqaDV9cYM">Video</a>
                            /
                            <a href="./bib/BaradelCophy_ICLR_20.txt">bibtex</a>
                        </p>
                        <p>  We introduce a new problem of counterfactual learning of object mechanics from visual input and a benchmark called CoPhy.
                        </p>
                    </td>
                </tr>     
                <tbody>
            </table>
