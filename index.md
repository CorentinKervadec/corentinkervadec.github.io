---
layout: default
---

üëã I defended my PhD on *Biases and Reasoning in Visual Question Answering* in December 2021, under the direction of [Christian Wolf](https://chriswolfvision.github.io/www/) and co-supervised by [Grigory Antipov](https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl) and [Moez Baccouche](https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl) at Orange Labs.

<!---üëã I am a PhD candidate at [INSA Lyon](https://www.insa-lyon.fr/) financed by [Orange Labs](https://www.orange.com/fr/accueil) in the [Imagine team (LIRIS)](https://liris.cnrs.fr/equipe/imagine) under the direction of [Christian Wolf](https://perso.liris.cnrs.fr/christian.wolf/).

My thesis is also co-supervised by [Grigory Antipov](https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl) and [Moez Baccouche](https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl) from Orange Labs.--->

üß† My work focuses on Deep Learning applied to Vision and Language, with a special interest on Visual Reasoning and Visual Question Answering (VQA).

<!---üì¢ **I am looking for a postdoc in ML. I would enjoy working on shortcut learning and/or reasoning models. Please, feel free to contact me ([CV](cv_kervadec_corentin_06_2021.pdf)).** --->

# News

:scroll: *September 2021*: 1 paper accepted at [NeurIPS2021](https://neurips.cc/)! [Supervising the Transfer of Reasoning Patterns in VQA](https://openreview.net/forum?id=kqYiS7HEWfZ)

ü•á *September 2021*: I have been selected as an outstanding reviewer for ICCV'21 (top 5% students)!

:scroll: *July 2021*: 1 paper accepted at [IEEE VIS2021](http://ieeevis.org/year/2021/welcome)! [VisQA: X-raying Vision and Language Reasoning in Transformers](https://arxiv.org/abs/2104.00926)

:man_teacher: *June 2021*: I presented a poster about biases and reasoning at the [VQA workshop](https://visualqa.org/workshop.html) at CVPR'21. Watch the [video](https://youtu.be/ABY2InY-RaE) and the [poster](posters/biases_reasoning_vqa.pdf)!

:man_teacher: *May 2021*: I was invited to give a talk about biases and reasoning in VQA at ["Devil is in the Deeptails"](https://project.inria.fr/ml3ri/dissemination-and-communication/deeptails/) ([slides](https://project.inria.fr/ml3ri/files/2021/06/slides-corentin.pdf) and [video](https://www.youtube.com/watch?v=i8BTiR8rs5Q)).

:man_teacher: *April 2021*: I gave a talk about VQA and visual reasoning at the GdR ISIS ["Explicabilit√© et Interpr√©tabilit√© des m√©thodes d'Intelligence Artificielle pour la classification et compr√©hension des sc√®nes visuelles"](http://www.gdr-isis.fr/index.php?page=compte-rendu&idreunion=446) meeting. Slides are available [here](https://github.com/CorentinKervadec/corentinkervadec.github.io/blob/master/slides/GdR_ISIS___Roses_Reasoning__slides_Kervadec_042021.pdf).

:scroll: *Mars 2021*: 2 papers accepted at [CVPR2021](http://cvpr2021.thecvf.com/)! ["Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?"](https://arxiv.org/abs/2006.05121) and ["How Transferable are Reasoning Patterns in VQA?"](https://arxiv.org/abs/2104.03656) (checkout our online demo [here](https://reasoningpatterns.github.io/)!)

:scroll: *June 2020*: New paper on Arxiv! ["Estimating semantic structure for the VQA answer space"](https://arxiv.org/abs/2006.05726)

:scroll: *January 2020*: One paper accepted at [ECAI20](http://ecai2020.eu/)! ["Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks."](https://arxiv.org/pdf/1912.03063.pdf)

:scroll: *May 2019*: One paper accepted at [IEEE FG2019](https://fg2019.org/)! ["The Many Variations of Emotion."](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8756560)

:man_student: *October 2018*: Sarting my PhD at INSA Lyon & Orange Labs under the direction of [Christian Wolf](https://perso.liris.cnrs.fr/christian.wolf/) and co-supervised by [Grigory Antipov](https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl) and [Moez Baccouche](https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl).

:scroll: *July 2018*: One paper accepted at the IAHFAR wotkshop hosted at [BMVC18](http://bmvc2018.org/index.html)! ["CAKE: Compact and Accurate K-dimensional representation of Emotion.](http://bmvc2018.org/contents/workshops/iahfar2018/0037.pdf)

:3rd_place_medal: *June 2018*: Rank 3th at the [Emotion in the Wild 2018 challenge](https://sites.google.com/view/emotiw2018) hosted at [ICMI18](https://icmi.acm.org/2018/)! ["An Occam's Razor View on Learning Audiovisual Emotion Recognition with Small Training Sets."](https://dl.acm.org/doi/pdf/10.1145/3242969.3264980)  

:man_student: *March 2018*, Starting a Master's internship at [Orange Labs](https://www.orange.com/fr/accueil).

# Publications
 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tbody>     
                    <b><p style="font-size:18px;"><a href="https://openreview.net/forum?id=kqYiS7HEWfZ"> Supervising the Transfer of Reasoning Patterns in VQA</a></p></b>
                    <p align="center"><img src="progqa/fig2_v4.png" alt="blind-date" width="80%">
                     <p>
                            <br>
                            <strong>Corentin Kervadec*</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf*</a>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="https://scholar.google.fr/citations?user=KOXeslUAAAAJ&hl">Madiha Nadri</a>,
                            <br>
                            <em>NeurIPS</em>, 2021 &nbsp;
                            <br>
                            <a href="https://openreview.net/pdf?id=kqYiS7HEWfZ">PDF</a>
                            /
                            <a href="https://openreview.net/forum?id=kqYiS7HEWfZ">OpenReview</a>
                        </p>
                        <p>  We propose a method for knowledge transfer in VQA based on a regularization term in our loss function, supervising the sequence of required reasoning operations. We provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses.
                        </p>
                 <tbody>     
                    <b><p style="font-size:18px;"><a href="https://arxiv.org/abs/2104.00926"> VisQA: X-raying Vision and Language Reasoning in Transformers</a></p></b>
                    <p align="center"><img src="visqa/visqa.png" alt="blind-date" width="80%">
                     <p>
                            <br>
                            <a href="https://theo-jaunet.github.io/">Theo Jaunet</a>,
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="https://romain.vuillemot.net/">Romain Vuillemot</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>IEEE VIS</em>, 2021 &nbsp;
                            <br>
                            <a href="https://arxiv.org/pdf/2104.00926.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/2104.00926">Arxiv</a>
                            /
                            <a href="https://github.com/Theo-Jaunet/VisQA">Github</a>
                            /
                            <a href="https://visqa.liris.cnrs.fr/">Online Demo!</a>
                        </p>
                        <p>  We introduce VisQA, a visual analytics tool that explores the question of reasoning vs. bias exploitation in Visual Question Answering systems. Try our interactive tool <a href="https://visqa.liris.cnrs.fr/">here</a>!
                        </p>
                <tbody>     
                    <b><p style="font-size:18px;"><a href="https://arxiv.org/abs/2104.03656"> How Transferable are Reasoning Patterns in VQA?</a></p></b>
                    <p align="center"><img src="reasoning_patterns/teaser.png" alt="blind-date" width="90%">
                     <p>
                            <br>
                            <strong>Corentin Kervadec*</strong>,
                            <a href="https://theo-jaunet.github.io/">Theo Jaunet*</a>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="https://romain.vuillemot.net/">Romain Vuillemot</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>CVPR</em>, 2021 &nbsp;
                            <br>
                            <a href="https://arxiv.org/pdf/2104.03656.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/2104.03656">Arxiv</a>
                            /
                            <a href="https://youtu.be/fN1m7b6u3-E">Video</a>
                            /
                            <a href="posters/poster_reasoning_v2.pdf">Poster</a>
                            /
                            <a href="https://reasoningpatterns.github.io/">Online Demo!</a>
                        </p>
                        <p>  Noise and uncertainties in visual inputs are the main bottleneck in VQA, preventing successful learning of reasoning capacities. In a deep analysis, we show that oracle models trained on noiseless visual data, tend to depend significantly less on bias exploitation (<a href="https://reasoningpatterns.github.io/">checkout our interactive tool</a>). In this, paper we demonstrate the feasability and the effectiveness of transfering learned reasoning patterns from oracle to real data based models.
                        </p>
                <tbody>     
                    <b><p style="font-size:18px;"><a href="https://128.84.21.199/pdf/2006.05121.pdf"> Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?</a></p></b>
                    <p align="center"><img src="gqa_ood/teaser-girl-v8.png" alt="blind-date" width="66%">
                     <p>
                            <br>
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>CVPR</em>, 2021 &nbsp;
                            <br>
                            <a href="https://arxiv.org/pdf/2006.05121.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/2006.05121">arXiv</a>
                            /
                            <a href="https://github.com/gqa-ood/GQA-OOD/tree/master/code">Code</a>
                            /
                            <a href="https://github.com/gqa-ood/GQA-OOD">Benchmark</a>
                            /
                            <a href="https://youtu.be/TAPS715WN7o">Video</a>
                            /
                            <a href="posters/poster_roses_v2.pdf">Poster</a>
                        </p>
                        <p>  We propose <a href="https://github.com/gqa-ood/GQA-OOD">GQA-OOD</a>, a new benchmark to evaluate VQA in out-of-distribution settings by reorganizing the GQA dataset, taylored for each sample (question group), targeting research in bias reduction in VQA.
                        </p>
                <tbody>
                   <b><p style="font-size:18px;"><a href="https://128.84.21.199/pdf/2006.05726.pdf">
                    Estimating semantic structure for the VQA answer space</a></p></b>
                   <p align="center"><img src="semantic_vqa/general.png" alt="blind-date"                                width="66%"></p>
                        <p>
                            <br>
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>Arxiv</em>, 2020 &nbsp;
                            <br>
                            <a href="https://arxiv.org/pdf/2006.05726.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/2006.05726">arXiv</a>
                        </p>
                        <p>  Semantic loss for VQA adding structure to the VQA answer space estimated from redundancy in annotations, questioning the classification approach to VQA.
                        </p>
                <tbody>
                 <b><p style="font-size:18px;"><a href="http://ecai2020.eu/papers/1241_paper.pdf">
                                Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks</a></p></b>
                 <p align="center"><img src="word_object_alignment/fig_teaser.png" alt="blind-date"
                                        width="66%"></p>
                        <p>
                            <br>
                            <strong>Corentin Kervadec</strong>,
                            <a href="https://scholar.google.fr/citations?user=CoOz8K0AAAAJ&hl">Grigory Antipov</a>,
                            <a href="https://scholar.google.fr/citations?user=olfpe-kAAAAJ&hl">Moez Baccouche</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>ECAI</em>, 2020 &nbsp;
                            <br>
                            <a href="http://ecai2020.eu/papers/1241_paper.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1912.03063">arXiv</a>
                            /
                            <a href="https://youtu.be/SW8J0G8AbgY">video</a>
                            /
                            <a href="./bib/KervadecWOA_ECAI_20.txt">bibtex</a>
                        </p>
                        <p>  We introduce a weakly supervised word-object alignment inside BERT-like Vision-Language encoders, allowing to model fine-grained entity relations and improve visual reasoning capabilities.
                        </p>   
                <tbody>
                 <b><p style="font-size:18px;"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8756560&tag=1">
                                The Many Variations of Emotion</a></p></b>
                 <p align="center"><img src="many_variations/many_variations.PNG" alt="blind-date"
                                        width="66%"></p>
                        <p>
                            <br>
                            <a href="https://scholar.google.fr/citations?user=ve7hYuQAAAAJ&">Valentin Vielzeuf</a>,
                            <strong>Corentin Kervadec</strong>,
                            St√©phane Pateux,
                            <a href="https://frederic-jurie.github.io/">Frederic Jurie</a>
                            <br>
                            <em>IEEE FG</em>, 2019 &nbsp;
                            <br>
                            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8756560&tag=1">PDF</a>
                            /
                            <a href="./bib/VielzeufMAny_FG_19.txt">bibtex</a>
                        </p> We present a novel approach for changing facial expression in images by the use of a continuous latent space of emotion.
                        <p>  
                        </p>   
                <tbody>
                 <b><p style="font-size:18px;"><a href="http://bmvc2018.org/contents/workshops/iahfar2018/0037.pdf">
                                CAKE: Compact and Accurate K-dimensional representation of Emotion</a></p></b>
                 <p align="center"><img src="cake/cake_viz.PNG" alt="blind-date"
                                        width="66%"></p>
                        <p>
                            <br>
                            <strong>Corentin Kervadec*</strong>, 
                            <a href="https://scholar.google.fr/citations?user=ve7hYuQAAAAJ&">Valentin Vielzeuf*</a>,
                            St√©phane Pateux,
                            <a href="https://lechervy.users.greyc.fr/">Alexis Lechervy,</a>
                            <a href="https://frederic-jurie.github.io/">Frederic Jurie</a>
                            <br>
                            <em>IAHFAR workshop (BMVC)</em>, 2018 &nbsp;
                            <br>
                            <a href="http://bmvc2018.org/contents/workshops/iahfar2018/0037.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1807.11215">arXiv</a>
                            / 
                            <a href="./bib/KervadecCake_IAHFAR_18.txt">bibtex</a>
                        </p> We propose CAKE, a 3-dimensional representation of emotion learned in a multi-domain fashion, achieving accurate emotion recognition on several public datasets
                        <p>  
                        </p>   
                <tbody>
                 <b><p style="font-size:18px;"><a href="https://dl.acm.org/doi/pdf/10.1145/3242969.3264980">
                                An occam's razor view on learning audiovisual emotion recognition with small training sets</a></p></b>
                        <p>
                            <a href="https://scholar.google.fr/citations?user=ve7hYuQAAAAJ&">Valentin Vielzeuf</a>,
                            <strong>Corentin Kervadec</strong>, 
                            St√©phane Pateux,
                            <a href="https://lechervy.users.greyc.fr/">Alexis Lechervy,</a>
                            <a href="https://frederic-jurie.github.io/">Frederic Jurie</a>
                            <br>
                            <em>EmotiW challenge (ICMI)</em>, 2018 &nbsp;
                            <br>
                            <a href="https://dl.acm.org/doi/pdf/10.1145/3242969.3264980">PDF</a>
                            / 
                            <a href="./bib/VielzeufOccam_ICMI_18.txt">bibtex</a>
                        </p>  A light-weight and accurate deep neural model for audiovisual emotion recognition. We ranked 3th at the Emotion in the Wild 2018 challenge.
                        <p>  
                        </p>   
                <tbody>
